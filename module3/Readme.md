# ğŸ“˜ Module 3: Information Retrieval in Production

> DeepLearning.AI_

---

## ğŸ“‹ Module Overview

- Approximate Nearest Neighbors (ANN) algorithms
- Hierarchical Navigable Small World (HNSW)
- Vector Databases: setup, indexing, search
- Chunking strategies and advanced techniques
- Query parsing: rewriting, NER, HyDE
- Cross-encoders, ColBERT
- Reranking pipelines
- Production best practices

---

## ğŸ” Approximate Nearest Neighbors (ANN)

- **KNN (Brute Force)**: Linear scan of all vectors â†’ slow at scale  
- **ANN**: Uses proximity graphs or index structures for sub-linear search  
  - Fast, but may not return absolute nearest neighbors  
- **Neighbor Graphs**: Nodes = vectors, edges connect nearest neighbors îˆ€fileciteîˆ‚turn2file7îˆ

---

## ğŸ§± Hierarchical Navigable Small World (HNSW)

1. **Layer 3**: ~10 random vectors, rough navigation  
2. **Layer 2**: ~100 vectors, intermediate navigation  
3. **Layer 1**: All vectors, precise nearest-neighbor search  
- Traversal: start at top layer, descend to layer 1 for best match  
- Complexity: roughly logarithmic in dataset size îˆ€fileciteîˆ‚turn2file19îˆ

---

## ğŸ“‚ Vector Databases

- Optimized storage and ANN search for high-dimensional data  
- Outperform relational databases for semantics-based retrieval  
- **Weaviate** used in this course îˆ€fileciteîˆ‚turn2file24îˆ

### Typical Operations

1. **Setup**: create collections with vectorizer configs  
2. **Load Documents**: batch import with error handling  
3. **Indexing**: build HNSW index for fast ANN  
4. **Search**: `near_text`, BM25, hybrid, filtered queries îˆ€fileciteîˆ‚turn2file28îˆ‚turn2file29îˆ

---

## ğŸ”— Complete Workflow

```mermaid
graph LR
  A[Configure DB] --> B[Load & Index Data]
  B --> C[Perform Searches]
```
îˆ€fileciteîˆ‚turn2file32îˆ

---

## ğŸ“‘ Chunking

- **Why?** Stay within token limits, improve relevance, send only useful context îˆ€fileciteîˆ‚turn2file34îˆ
- **Fixed-size**: e.g., 250 characters per chunk; optionally with overlap for context continuity îˆ€fileciteîˆ‚turn2file39îˆ
- **Overlapping**: chunks overlap by 10â€“20% to avoid cutting sentences îˆ€fileciteîˆ‚turn2file40îˆ
- **Recursive splitting**: split on newline or semantic markers for better structure îˆ€fileciteîˆ‚turn2file41îˆ

---

## ğŸ§  Advanced Chunking Techniques

- **Semantic chunking**: group sentences by embedding similarity, split when semantic distance exceeds threshold îˆ€fileciteîˆ‚turn2file46îˆ
- **LLM-based chunking**: prompt an LLM to segment document into concept-based chunks îˆ€fileciteîˆ‚turn2file50îˆ
- **Context-aware**: prepend external metadata/context to each chunk for improved retrieval and generation îˆ€fileciteîˆ‚turn2file51îˆ

**Choosing an Approach**:
- Start with fixed-size or recursive splits  
- Experiment with semantic/LLM-based for higher precision  
- Add context-aware as a first improvement îˆ€fileciteîˆ‚turn2file53îˆ

---

## ğŸ” Query Parsing

### Query Rewriting

- Clean up user prompts, clarify ambiguity, add domain terminology/synonyms  
- Example: rewrite patient scenario for medical DB search îˆ€fileciteîˆ‚turn2file57îˆ

### Named Entity Recognition (NER)

- Extract entities (places, dates, people, organizations) to refine search filters  
- GLINER demo: tag â€œThe Great Gatsbyâ€ as BOOK, â€œ1920sâ€ as DATE îˆ€fileciteîˆ‚turn2file60îˆ

### HyDE (Hypothetical Document Embeddings)

- Generate a hypothetical â€œidealâ€ document from the prompt  
- Embed that document for ANN search, matching doc-to-doc instead of prompt-to-doc îˆ€fileciteîˆ‚turn2file62îˆ

---

## âš–ï¸ Cross-Encoders & ColBERT

### Bi-Encoder

- Embed prompt and docs separately, use ANN for initial retrieval îˆ€fileciteîˆ‚turn2file64îˆ

### Cross-Encoder

- Concatenate prompt+doc, score with deeper model  
- High quality but too slow for large corpora; use for reranking top-k îˆ€fileciteîˆ‚turn2file65îˆ

### ColBERT

- Late interaction on per-token embeddings  
- Balances speed (pre-computed tokens) and quality (token-level matching)  
- Storage-heavy (vector per token) but high relevance îˆ€fileciteîˆ‚turn2file68îˆ

---

## ğŸ”„ Reranking

- Pipeline: initial retrieval (bi-encoder/hybrid) â†’ rerank top results (cross-encoder or LLM scoring) â†’ final docs to LLM îˆ€fileciteîˆ‚turn2file75îˆ
- Improves precision for LLM generation, minimal additional latency îˆ€fileciteîˆ‚turn2file79îˆ

---

## ğŸ“Œ Key Takeaways

- **ANN & HNSW** for sub-linear retrieval  
- **Vector DBs** streamline indexing and search  
- **Chunking**: foundational step for RAG workflows  
- **Query parsing**: rewriting, NER, HyDE enhance recall/precision  
- **Cross/ColBERT & reranking**: trade quality vs. efficiency

---
